{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MIT dataset/230\n",
      "Extracting MIT dataset/231\n",
      "Extracting MIT dataset/232\n",
      "Extracting MIT dataset/233\n",
      "Extracting MIT dataset/234\n",
      "Extracting MIT dataset/200\n",
      "Extracting MIT dataset/201\n",
      "Extracting MIT dataset/202\n",
      "Extracting MIT dataset/203\n",
      "Extracting MIT dataset/205\n",
      "Extracting MIT dataset/207\n",
      "Extracting MIT dataset/208\n",
      "Extracting MIT dataset/209\n",
      "Extracting MIT dataset/210\n",
      "Extracting MIT dataset/212\n",
      "Extracting MIT dataset/213\n",
      "Extracting MIT dataset/214\n",
      "Extracting MIT dataset/215\n",
      "Extracting MIT dataset/217\n",
      "Extracting MIT dataset/219\n",
      "Extracting MIT dataset/220\n",
      "Extracting MIT dataset/221\n",
      "Extracting MIT dataset/222\n",
      "Extracting MIT dataset/223\n",
      "Extracting MIT dataset/100\n",
      "Extracting MIT dataset/101\n",
      "Extracting MIT dataset/102\n",
      "Extracting MIT dataset/103\n",
      "Extracting MIT dataset/104\n",
      "Extracting MIT dataset/105\n",
      "Extracting MIT dataset/106\n",
      "Extracting MIT dataset/107\n",
      "Extracting MIT dataset/108\n"
     ]
    }
   ],
   "source": [
    "#joshua Sweet, Lauren Scott, Sravani Ravula\n",
    "\n",
    "#imports. Some might need to be installed before running.\n",
    "#%matplotlib notebook \n",
    "from sklearn.metrics import classification_report as cr\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from find_peak import find_peaks # From external file \"find_peak.py\"\n",
    "from sklearn.svm import SVC\n",
    "from wfdb import processing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import wfdb\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "# A list of MIT datasets stored in \"./MIT Datset/\"\n",
    "fileSet = {100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113,\n",
    "           114, 115, 116, 117, 118, 119, 121, 122, 123, 124, 200, 201, 202,\n",
    "           203, 205, 207, 208, 209, 210, 212, 213, 214, 215, 217, 219, 220,\n",
    "           221, 222, 223, 228, 230, 231, 232, 233, 234}\n",
    "\n",
    "# A dictionary containing the mapping of letter annotation labels to their respective numbers\n",
    "labels = {\n",
    "    \" \": 0, \"N\": 0, \"L\": 2, \"R\": 3, \n",
    "    \"a\": 4, \"V\": 5, \"F\": 6, \"J\": 7, \n",
    "    \"A\": 8, \"S\": 9,\"E\": 10, \"j\": 11, \n",
    "    \"/\": 12, \"Q\": 13, \"~\": 14, \"|\": 16, \n",
    "    \"s\": 18, \"T\": 19, \"*\": 20, \"D\": 21, \n",
    "    \"\\\"\": 22, \"=\": 23, \"p\": 24, \"B\": 25, \n",
    "    \"^\": 26, \"t\": 27, \"+\": 28, \"u\": 29, \n",
    "    \"?\": 30, \"!\": 31, \"[\": 32, \"]\": 33, \n",
    "    \"e\": 34, \"n\": 35, \"@\": 36, \"x\": 37, \n",
    "    \"f\": 38, \"(\": 39\n",
    "}\n",
    "    \n",
    "\n",
    "def main():\n",
    "    for file in fileSet:\n",
    "        try:\n",
    "            record = str (\"MIT dataset/\" + str (file))\n",
    "            print (\"Extracting \" + record)\n",
    "        \n",
    "            # Extract the wfdb data revelent to the current MIT record being looked at\n",
    "            sig, fields = wfdb.rdsamp (record_name = record, channels = [0])\n",
    "            ann = wfdb.rdann (record_name = record, extension = \"atr\")\n",
    "        \n",
    "            # Gather the annotation and value data from the wfdb fields returned\n",
    "            x, y = gather_data (sig, ann)\n",
    "            plot_figure (file) # Make a graph of the data\n",
    "          \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    # Store all the data gathered into a file\n",
    "    create_data ([\"features\", \"annotations\"], [x, y])\n",
    "    print (\"Data Creation Done\")\n",
    "    \n",
    "    # Train the data which was created into a training set\n",
    "    trainX, testX, trainY, testY = train_data (\"features\", \"annotations\")\n",
    "    print (\"Training Set Done\")\n",
    "    \n",
    "    # Apply the training set to an SVM module.\n",
    "    svcType = SVC (kernel = \"linear\")\n",
    "    svcType.fit (trainX, trainY)\n",
    "    predicition = svcType.predict (testX)\n",
    "    print (\"Predicition Done\")\n",
    "    \n",
    "    # Display the relevent information\n",
    "    print (cr (testY, predicition))\n",
    "    print (\"Confusion Matrix: \", end = \"\")\n",
    "    confusionMatrix = cm (testY, predicition)\n",
    "    print (confusionMatrix)\n",
    "    print (\"Predicition Values: \", end = \"\")\n",
    "    print (predicition)\n",
    "\n",
    "    resultingLabels = list (set (testY + trainY))\n",
    "    \n",
    "    try:\n",
    "        resultingLabels.sort ()\n",
    "        plt.figure (figsize = (15, 16))\n",
    "    \n",
    "        # Creates a heatamap of the confusion matrix\n",
    "        confusionMatrixData = pd.DataFrame (confusionMatrix, index = resultingLabels, columns = resultingLabels)\n",
    "        sb.heatmap (confusionMatrixData)\n",
    "        plt.show ()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return 0\n",
    "    \n",
    "\n",
    "def plot_figure(file):\n",
    "    # Get the wfdb record and annotation for that dataset, sampling each to be 999 datapoints\n",
    "    record = wfdb.rdrecord (\"MIT dataset/\" + str (file), sampto = 999)\n",
    "    ann = wfdb.rdann (\"MIT dataset/\" + str (file), \"atr\", sampto = 999)\n",
    "    sig, fields = wfdb.rdsamp(\"MIT dataset/\" + str (file), sampto = 999)\n",
    "    \n",
    "    peaks = find_peaks (sig, fields)\n",
    "    \n",
    "    # Create the matplotlib figure\n",
    "    figure = wfdb.plot_wfdb (record = record, annotation = ann, plot_sym = True, time_units = \"seconds\", title = \"Dataset \" + str (file), return_fig = True)\n",
    "    figure.set_size_inches (9, 8)\n",
    "    \n",
    "    # Loop across each axis the figure has\n",
    "   # for j in range (0, len (figure.axes)):\n",
    "    axis = figure.sca (figure.axes[0])\n",
    "    data = axis.lines[0]\n",
    "            \n",
    "    ypoints = data.get_ydata ()\n",
    "    xpoints = data.get_xdata ()\n",
    "    \n",
    "       # if j\n",
    "            # Loop across each local max peak\n",
    "    for i in range (0, len (peaks)):\n",
    "            \n",
    "        # Plot a point at the current point\n",
    "        plot_points (xpoints.tolist ()[peaks[i]], ypoints.tolist ()[peaks[i]], 0.05, -0.075)\n",
    "        if i > 0:      \n",
    "            # Plot a line from the previous peak to the current peak\n",
    "             plt.plot ([xpoints.tolist ()[peaks[i - 1]], xpoints.tolist ()[peaks[i]]], \n",
    "                        [ypoints.tolist ()[peaks[i - 1]], ypoints.tolist ()[peaks[i]]])\n",
    "    \n",
    "def plot_points(x, y, shiftx, shifty):\n",
    "    # Plot a point and the cords of that point at postion (x,y)\n",
    "    plt.plot (x, y, \".\")\n",
    "    plt.text (x + shiftx, y + shifty, \"(\" + str (round (x, 3)) + \",\" + str (round (y, 3)) +  \")\",fontsize=6)\n",
    "        \n",
    "\n",
    "def gather_data(signal, annotations, x = [], y = []):\n",
    "    # Loop through every annotation label extracted\n",
    "    for i in range (annotations.sample.size - 1):\n",
    "        # For each annotation label, take the index of that current label, and gather each time sample\n",
    "        # from the range of i - 180 to i + 180\n",
    "        times = []\n",
    "        \n",
    "        # If the index of i - 180 would be negative, just take the range of 0 to 360\n",
    "        if annotations.sample[i] - 180 < 0:\n",
    "            for j in range (0,360):\n",
    "                times.append (float (signal[j]))\n",
    "        else:\n",
    "            for j in range ((annotations.sample[i] - 180), (annotations.sample[i] + 180)):\n",
    "                times.append (float (signal[j]))\n",
    "        \n",
    "        x.append (times)\n",
    "        y.append(annotations.symbol[i])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def create_data(filename, data):\n",
    "    # Create each file and store the relevent data inside of it\n",
    "    for i in range (0, 2):\n",
    "        with open(\"MIT dataset/\" + filename[i], \"wb+\") as file:\n",
    "            pickle.dump(data[i], file)\n",
    "            file.close ()\n",
    "    \n",
    "def load_data(features, annotations):\n",
    "    return pickle.load (open(\"MIT dataset/\" + features, \"rb\")), pickle.load (open(\"MIT dataset/\" + annotations, \"rb\"))\n",
    "\n",
    "def train_data(features, annotations):\n",
    "    xValues = []\n",
    "    yValues = []\n",
    "    x, y = load_data (features, annotations)\n",
    "    # Create a list of every y annotation converted to its number value\n",
    "    y = list (map (convert_annotations, y))\n",
    "    \n",
    "    # MinMax the list of times related to each annotation label, then fit and transform the set.\n",
    "    scaler = MinMaxScaler (copy = True, feature_range = (0, 39))\n",
    "    scaler.fit (x)\n",
    "    scaler.transform (x)\n",
    "    \n",
    "    # Loop through every annotation value\n",
    "    for i in range (len (y)):\n",
    "        # If the annotation value is 0. (Which means its a normal beat)\n",
    "        if not y[i]:\n",
    "            # Only add it to the training set if a random number from 0.95 to 1 is generated.\n",
    "            # This is done to filter out high amounts of normal beats\n",
    "            if random.random() > 0.95:\n",
    "                yValues.append (y[i])\n",
    "                xValues.append (x[i])\n",
    "        # Else it is a beat which is abnormal, add it to the training set.\n",
    "        else:\n",
    "            yValues.append (y[i])\n",
    "            xValues.append (x[i])\n",
    "    return train_test_split (xValues, yValues)\n",
    "\n",
    "def convert_annotations(y):\n",
    "    try:\n",
    "        return labels[y[0]]\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
